{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Polynomial Overfitting Example (Section 7.1)\n",
    "\n",
    "**The Problem**: More parameters always improve fit to training data, but this doesn't mean better predictions!\n",
    "\n",
    "**Example**: Primate brain volume vs body mass\n",
    "- 7 species (small dataset!)\n",
    "- Fit polynomials of degree 1, 2, 3, 4, 5, 6\n",
    "- Watch R² improve while models become absurd\n",
    "\n",
    "**Key Lesson**: We need measures beyond R² to evaluate models!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "from src.quap import quap\n",
    "\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print('✓ Imports loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Load Primate Brain Data\n",
    "\n",
    "From Statistical Rethinking - brain volume and body mass for 7 primate species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Note: This is a small dataset from the book\n",
    "# Species: Different primates\n",
    "# brain: Brain volume (cc)\n",
    "# mass: Body mass (kg)\n",
    "\n",
    "# Create the dataset (from the book)\n",
    "data = pd.DataFrame({\n",
    "    'species': ['afarensis', 'africanus', 'habilis', 'boisei',\n",
    "                'rudolfensis', 'ergaster', 'sapiens'],\n",
    "    'brain': [438, 452, 612, 521, 752, 871, 1350],\n",
    "    'mass': [37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5]\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {len(data)} primate species\")\n",
    "print(\"\\nData:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the variables\n",
    "brain_std = (data['brain'] - data['brain'].mean()) / data['brain'].std()\n",
    "mass_std = (data['mass'] - data['mass'].mean()) / data['mass'].std()\n",
    "\n",
    "print(\"Variables standardized\")\n",
    "print(f\"Brain (standardized): mean={brain_std.mean():.2f}, std={brain_std.std():.2f}\")\n",
    "print(f\"Mass (standardized): mean={mass_std.mean():.2f}, std={mass_std.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Fit Polynomial Models of Increasing Degree\n",
    "\n",
    "We'll fit 6 models:\n",
    "1. **Linear** (degree 1): μ = α + β₁x\n",
    "2. **Quadratic** (degree 2): μ = α + β₁x + β₂x²\n",
    "3. **Cubic** (degree 3): μ = α + β₁x + β₂x² + β₃x³\n",
    "4. **Quartic** (degree 4): μ = α + β₁x + ... + β₄x⁴\n",
    "5. **Quintic** (degree 5): μ = α + β₁x + ... + β₅x⁵\n",
    "6. **Sextic** (degree 6): μ = α + β₁x + ... + β₆x⁶\n",
    "\n",
    "**Prediction**: R² will improve with each degree, but models will become absurd!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit polynomial of given degree\n",
    "def fit_polynomial(x, y, degree):\n",
    "    \"\"\"\n",
    "    Fit a polynomial regression of given degree using quap.\n",
    "    \n",
    "    Model: y ~ Normal(μ, σ)\n",
    "           μ = α + β₁x + β₂x² + ... + βₙxⁿ\n",
    "    \"\"\"\n",
    "    # Create polynomial features\n",
    "    X_poly = np.column_stack([x**i for i in range(1, degree + 1)])\n",
    "    \n",
    "    # Define negative log posterior\n",
    "    def neg_log_posterior(params):\n",
    "        alpha = params[0]\n",
    "        betas = params[1:degree+1]\n",
    "        log_sigma = params[degree+1]\n",
    "        sigma = np.exp(log_sigma)\n",
    "        \n",
    "        # Linear predictor\n",
    "        mu = alpha + np.sum(X_poly * betas, axis=1)\n",
    "        \n",
    "        # Log likelihood\n",
    "        log_lik = np.sum(stats.norm.logpdf(y, loc=mu, scale=sigma))\n",
    "        \n",
    "        # Priors\n",
    "        log_prior = (stats.norm.logpdf(alpha, 0, 0.2) +\n",
    "                    np.sum(stats.norm.logpdf(betas, 0, 0.5)) +\n",
    "                    stats.expon.logpdf(sigma, scale=1))\n",
    "        \n",
    "        return -(log_lik + log_prior + log_sigma)\n",
    "    \n",
    "    # Initial parameters: alpha, beta1, ..., betan, log_sigma\n",
    "    initial = [0] * (degree + 2)\n",
    "    param_names = ['alpha'] + [f'beta{i}' for i in range(1, degree+1)] + ['log_sigma']\n",
    "    \n",
    "    # Fit with quap\n",
    "    fit = quap(neg_log_posterior, initial, param_names)\n",
    "    fit.transform_param('log_sigma', 'sigma', np.exp)\n",
    "    \n",
    "    return fit\n",
    "\n",
    "print(\"✓ Polynomial fitting function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all 6 models\n",
    "models = {}\n",
    "degrees = range(1, 7)\n",
    "\n",
    "for deg in degrees:\n",
    "    print(f\"\\nFitting polynomial degree {deg}...\")\n",
    "    models[deg] = fit_polynomial(mass_std.values, brain_std.values, deg)\n",
    "    print(f\"  ✓ Converged: {models[deg].success}\")\n",
    "\n",
    "print(\"\\n✓ All models fitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Compute R² for Each Model\n",
    "\n",
    "**R²** measures proportion of variance explained.\n",
    "\n",
    "**Watch**: R² will increase monotonically as we add parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute R² for each model\n",
    "def compute_r_squared(model, x, y, degree):\n",
    "    \"\"\"\n",
    "    Compute R² for a fitted polynomial model.\n",
    "    \"\"\"\n",
    "    # Get coefficients\n",
    "    coefs = model.coef()\n",
    "    alpha = coefs['alpha']\n",
    "    betas = [coefs[f'beta{i}'] for i in range(1, degree+1)]\n",
    "    \n",
    "    # Predictions\n",
    "    X_poly = np.column_stack([x**i for i in range(1, degree + 1)])\n",
    "    y_pred = alpha + np.sum(X_poly * betas, axis=1)\n",
    "    \n",
    "    # R²\n",
    "    ss_res = np.sum((y - y_pred)**2)\n",
    "    ss_tot = np.sum((y - y.mean())**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "# Compute R² for all models\n",
    "r_squared_values = {}\n",
    "for deg in degrees:\n",
    "    r_squared_values[deg] = compute_r_squared(models[deg], \n",
    "                                              mass_std.values, \n",
    "                                              brain_std.values, \n",
    "                                              deg)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Degree': list(degrees),\n",
    "    'Parameters': [deg + 2 for deg in degrees],  # +2 for alpha and sigma\n",
    "    'R²': [r_squared_values[deg] for deg in degrees]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison (by R²):\")\n",
    "print(\"=\"*50)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*50)\n",
    "print(\"\\n⚠️ Notice: R² increases monotonically!\")\n",
    "print(\"   But are higher degree models really better?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Visualize the Fits - Recreate Figure 7.1\n",
    "\n",
    "Let's plot all 6 polynomial fits to see how they become increasingly wiggly and absurd!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions for smooth curves\n",
    "x_plot = np.linspace(mass_std.min() - 0.5, mass_std.max() + 0.5, 100)\n",
    "\n",
    "# Function to generate predictions\n",
    "def predict_polynomial(model, x, degree):\n",
    "    coefs = model.coef()\n",
    "    alpha = coefs['alpha']\n",
    "    betas = [coefs[f'beta{i}'] for i in range(1, degree+1)]\n",
    "    \n",
    "    X_poly = np.column_stack([x**i for i in range(1, degree + 1)])\n",
    "    y_pred = alpha + np.sum(X_poly * betas, axis=1)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Plot all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, deg in enumerate(degrees):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot data\n",
    "    ax.scatter(mass_std, brain_std, s=100, alpha=0.8, \n",
    "              edgecolor='black', linewidth=1.5, zorder=3)\n",
    "    \n",
    "    # Plot polynomial fit\n",
    "    y_plot = predict_polynomial(models[deg], x_plot, deg)\n",
    "    ax.plot(x_plot, y_plot, 'b-', linewidth=2, alpha=0.7, zorder=2)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Body mass (standardized)', fontsize=11)\n",
    "    ax.set_ylabel('Brain volume (standardized)', fontsize=11)\n",
    "    ax.set_title(f'Degree {deg} (R² = {r_squared_values[deg]:.3f})\\n' + \n",
    "                f'{deg + 2} parameters',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    \n",
    "    # Add annotation for absurd models\n",
    "    if deg >= 5:\n",
    "        ax.text(0.05, 0.95, '⚠️ Overfitting!', \n",
    "               transform=ax.transAxes, fontsize=10,\n",
    "               verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.suptitle('Polynomial Overfitting: More Parameters ≠ Better Model', \n",
    "            fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  • Degree 1-2: Reasonable fits\")\n",
    "print(\"  • Degree 3-4: Starting to wiggle\")\n",
    "print(\"  • Degree 5-6: Absurdly wiggly, clearly overfitting!\")\n",
    "print(\"\\n  Despite improving R², higher degrees are worse models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## The Problem: R² Always Increases!\n",
    "\n",
    "Let's visualize how R² changes with model complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot R² vs number of parameters\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "n_params = [deg + 2 for deg in degrees]  # +2 for alpha and sigma\n",
    "r2_values = [r_squared_values[deg] for deg in degrees]\n",
    "\n",
    "ax.plot(n_params, r2_values, 'o-', linewidth=2, markersize=10, \n",
    "       color='steelblue', label='R² (training fit)')\n",
    "ax.axhline(1.0, color='red', linestyle='--', alpha=0.5, label='Perfect fit')\n",
    "\n",
    "ax.set_xlabel('Number of Parameters', fontsize=12)\n",
    "ax.set_ylabel('R² (Proportion of Variance Explained)', fontsize=12)\n",
    "ax.set_title('R² Always Increases with More Parameters\\n(But this doesn\\'t mean better predictions!)', \n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "# Annotate the problem\n",
    "ax.annotate('Training fit improves...', \n",
    "           xy=(8, r2_values[-1]), xytext=(7, 0.6),\n",
    "           arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "           fontsize=11, color='red', fontweight='bold')\n",
    "\n",
    "ax.annotate('...but predictions\\nmay get worse!', \n",
    "           xy=(8, r2_values[-1]), xytext=(9, 0.4),\n",
    "           arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "           fontsize=11, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n⚠️ THE PROBLEM:\")\n",
    "print(\"   R² is NOT a good measure of model quality!\")\n",
    "print(\"   It always improves with more parameters.\")\n",
    "print(\"\\n✓ WE NEED:\")\n",
    "print(\"   - Measures that penalize complexity\")\n",
    "print(\"   - Out-of-sample validation\")\n",
    "print(\"   - Information criteria (WAIC, LOO)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Lessons\n",
    "\n",
    "### 1. R² is Misleading\n",
    "- R² always increases (or stays same) with more parameters\n",
    "- Perfect R² = 1.0 with N parameters for N data points\n",
    "- **But** this doesn't mean good predictions!\n",
    "\n",
    "### 2. Visual Inspection Reveals Absurdity\n",
    "- Degree 5-6 polynomials are clearly ridiculous\n",
    "- Wiggly curves that fit noise, not signal\n",
    "- Would make terrible predictions on new species\n",
    "\n",
    "### 3. We Need Better Measures\n",
    "To evaluate models properly, we need:\n",
    "\n",
    "**Option 1: Information Criteria**\n",
    "- WAIC, AIC, DIC\n",
    "- Balance fit vs complexity automatically\n",
    "- Coming in next sections!\n",
    "\n",
    "**Option 2: Cross-Validation**\n",
    "- Test on held-out data\n",
    "- Direct measure of predictive accuracy\n",
    "- Gold standard but expensive\n",
    "\n",
    "**Option 3: Regularization**\n",
    "- Skeptical priors that resist complexity\n",
    "- Built into the model itself\n",
    "\n",
    "### 4. The Overfitting Paradox\n",
    "> **\"Better fit to training data can mean worse predictions!\"**\n",
    "\n",
    "This is why we can't just maximize R² or minimize training error.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the following notebooks, we'll learn:\n",
    "1. **Information theory** - Why overfitting happens (entropy, KL divergence)\n",
    "2. **WAIC** - How to compute and use it for model comparison\n",
    "3. **Cross-validation** - Direct out-of-sample testing\n",
    "4. **Regularization** - Using priors to prevent overfitting\n",
    "\n",
    "These tools will help us choose models wisely!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
